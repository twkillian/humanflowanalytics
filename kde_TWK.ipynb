{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from math import *\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/train.txt', dtype=int, delimiter=',',skiprows=1)\n",
    "orig_data = np.copy(data)\n",
    "num_sensors = data.shape[1]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_data(data, sensor, time_start, time_end):\n",
    "    # Get starting index and ending index, derived from time_start and time_end\n",
    "    st_day, en_day = time_start/10000, time_end/10000\n",
    "    st_hour, en_hour = (time_start%10000)/100, (time_end%10000)/100\n",
    "    st_min, en_min = time_start%100, time_end%100\n",
    "    st_index = (st_day-1)*1440 + st_hour*60 + st_min\n",
    "    en_index = (en_day-1)*1440 + en_hour*60 + en_min\n",
    "    \n",
    "    return np.sum(data[st_index:en_index+1,sensor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get nearest neighbors of each piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/sensor-coordinates.txt') as f:\n",
    "    coords = list()\n",
    "    reader = csv.reader(f)\n",
    "    reader.next()\n",
    "    for row in reader:\n",
    "        coords.append((float(row[1]), float(row[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(p1, p2):\n",
    "    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nearest(coords, r=10):\n",
    "    nearest = list()\n",
    "    for i in range(56):\n",
    "        temp = list()\n",
    "        for j in range(56):\n",
    "            if i != j:\n",
    "                if dist(coords[i], coords[j]) < r:\n",
    "                    temp.append(j+1)\n",
    "        nearest.append((i+1, tuple(temp)))\n",
    "    return nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = get_nearest(coords,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timestamps = np.unique(data[:,0]%10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create intervals to sample over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window_size = 20\n",
    "intervals=[]\n",
    "for ii in range(0,len(timestamps)-window_size+1,10):\n",
    "    intervals.append(timestamps[ii:ii+window_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to estimate the distribution for each 20 minute interval (maybe less, 5 or 10 minutes might make more sense) for each sensor. \n",
    "\n",
    "From this distribution, you can randomly sample a specified number of times and then take the average of this sample to fill in the missing data. \n",
    "\n",
    "After you've filled the missing data you can then take a spatial average (using the nearest neighbors of each sensor that has missing data at that time step), giving the neighbors a higher weighting. \n",
    "\n",
    "Do this form of median filtering a couple of times. Check and see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "sampled_flow_by_interval = np.zeros((len(intervals),num_sensors))\n",
    "ival_count = 0\n",
    "for intval in intervals:\n",
    "    print ival_count\n",
    "    for i_sens in range(num_sensors):\n",
    "        sensor = data[:, [0, i_sens+1]]\n",
    "        sensor = sensor[sensor[:, 1] > -1]\n",
    "        sensor[:, 0] %= 10000\n",
    "        X = list()\n",
    "        for time in intval:\n",
    "            filtered = sensor[sensor[:, 0] == time]\n",
    "            if filtered.shape[0] != 0:\n",
    "                choice = np.random.choice(np.arange(filtered.shape[0]))\n",
    "\n",
    "                X.append(filtered[:,1])\n",
    "        X = list(itertools.chain.from_iterable(X))\n",
    "        \n",
    "        grid = GridSearchCV(KernelDensity(),\n",
    "                    {'bandwidth': np.logspace(-1, 1, 10)},\n",
    "                    cv=20) # 20-fold cross-validation\n",
    "        grid.fit(np.array(X).reshape(len(X), 1))\n",
    "        kde = grid.best_estimator_\n",
    "        \n",
    "        # Sample from kde and find expected value\n",
    "        sampled_entry = np.round(np.mean(np.abs(kde.sample(250))))\n",
    "        \n",
    "        # Place sampled entry into look-up matrix\n",
    "        sampled_flow_by_interval[ival_count,i_sens] = sampled_entry\n",
    "    ival_count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Place interval sampling into missing data locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sensor_list = np.linspace(1,56,56)\n",
    "for i in range(data.shape[0]):\n",
    "    temp = data[i,1:]\n",
    "    missing_sensors = sensor_list[temp==-1]\n",
    "    if len(missing_sensors) == 0:\n",
    "        continue\n",
    "\n",
    "    time = data[i,0]%10000\n",
    "    \n",
    "    sample_intervals = []\n",
    "    counter = 0\n",
    "    for intval in intervals:\n",
    "        if time in intval:\n",
    "            sample_intervals.append(counter)\n",
    "        counter += 1\n",
    "        \n",
    "    for i_sens in missing_sensors:\n",
    "        data[i,int(i_sens-1)] = np.mean(sampled_flow_by_interval[sample_intervals,int(i_sens-1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take spatial average twice (accounts for neighboring sensors being offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for repeat in range(2):\n",
    "    for i in range(data.shape[0]):\n",
    "        temp = data[i,1:]\n",
    "        missing_sensors = sensor_list[temp==-1]\n",
    "        if len(missing_sensors)==0:\n",
    "            continue\n",
    "        for i_sens in missing_sensors:\n",
    "            neighbors = n[int(i_sens-1)][1]\n",
    "            num_neighbors = len(neighbors)\n",
    "            weighting = float(2*num_neighbors + 1)\n",
    "        \n",
    "            data[i,i_sens] = (2/weighting)*(np.sum(data[i,neighbors])) + data[i,int(i_sens)]/weighting\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query sampled data set and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/test.txt') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    reader.next()\n",
    "    with open ('cF_submission.txt', 'w') as f:\n",
    "        counter = 1\n",
    "        f.write('Index,Count\\n')\n",
    "        for row in reader:\n",
    "            f.write('{},{}\\n'.format(counter, query_data(data,int(row[0][1:]),int(row[1]),int(row[2]))))\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
